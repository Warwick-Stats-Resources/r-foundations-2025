{
  "hash": "098c6b0dc3635db6bf11c4125f3ea132",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Simulation and Optimisation\nsubtitle: R Foundations 2024\nauthor: Ella Kaye, Department of Statistics\ndate: 2024-11-07\ndate-format: long\nformat: warwickpres-revealjs\n---\n\n## Overview\n\n-   Numerical precision\n\n-   Random numbers\n\n-   Simulation\n\n-   Optimisation\n\n\nThis material is largely reproduced from [Ruth Ripley's 2013 APTS course](https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf)\n\n# Numerical precision {.inverse}\n\n## How R stores numbers\n\nSome basic understanding is needed of how R stores and does arithmetic on numbers to avoid being surprised by the results it gives. \n\nFrom the R FAQ:\n\n> The only numbers that can be represented exactly in R’s numeric type are integers and fractions whose denominator is a power of 2. Other numbers have to be rounded to (typically) 53 binary digits accuracy. As a result, two floating point numbers will not reliably be equal unless they have been computed by the same algorithm, and not always even then. \n\n## Example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\na <- sqrt(2)\na * a == 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\na * a - 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.440892e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(a * a, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nThe function `all.equal()` compares two objects using a numeric tolerance of `.Machine$double.eps^0.5`.[^1] If you want much greater accuracy than this you will need to consider error propagation carefully.\n\n[^1]: `.Machine$double.eps` is the smallest positive floating-point number `x` such that `1 + x != 1` on the machine R is running on.\n\n## Numerical consideration resources\n\nFor more information, see e.g. \n\n- David Goldberg (1991), “What Every Computer Scientist Should Know About Floating-Point Arithmetic”, *ACM Computing Surveys*, **23/1**, 5–48, also available via <https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html>{target=\"_blank\"}.\n\n- Ruth Ripley's [numerical considerations](https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Numerics.pdf){target=\"_blank} notes for APTS 2013.\n\n# Simulation {.inverse}\n\n## Uses of simulation\n\n-   Statistical models are often mathematically intractable\n\n-   Generate multiple samples for a model by simulation\n\n-   Use these samples to investigate the behaviour of the model\n\n-   Need realisations of random variables with various different distributions, our *random numbers*\n\n- Details of how to use the random numbers to create samples will not be considered here\n\n## Random number generation\n\n-   Random numbers calculated on a computer are not random.\n\n-   They are *pseudo-random*, following a predicted sequence, but in the short-term (i.e. anything but the *very* long-term) will appear to be random.\n\n-   This is useful, as two sets of random numbers of the same size from the same generator using the same initial value will be exactly the same. **This is crucial for reproducibility.**\n\n-   R provides functions for generating random samples from standard distibutions.\n\n## Random seeds in R\n\n-   Each time a random number is required, R will use and update a variable called `.Random.seed` which is in your workspace.\n-   At the first use, if `.Random.seed` does not exist, one will be created, with a value generated from the time.\n-   The function `set.seed(n)` will set `.Random.seed` to a value derived from the argument `n`.\n-   Use `set.seed()` to repeat the same sequence of random numbers.\n\n## Random number generators in R\n-   There are various different generators available. R will use the one in use at the start of your session unless you alter it, even if you delete `.Random.seed`. For safety, you can specify the kind on calls to `set.seed`. Use `set.seed(n, kind=\"default\")` to ensure you are using R's default.\n-   It is possible to save and restore `.Random.seed` within your functions, but take care with scope.\n- Note that random number generation changed in R version 4.0.0, so you might get different outputs even with the same seed when using versions before and after that release.\n\n## `set.seed()` example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  5  8  7  6  9  2  1  4 10  3\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  9  4  7  1  2  5  3 10  6  8\n```\n\n\n:::\n\n```{.r .cell-code}\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3  1  5  8  2  6 10  9  4  7\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  9  4  7  1  2  5  3 10  6  8\n```\n\n\n:::\n:::\n\n\n## `sample()`\n\nThe `sample()` function re-samples from a data vector, with or without replacement.\n\n`sample(x, size, replace = FALSE, prob = NULL)`\n\n|           |                                                                                         |\n|-----------------|-------------------------------------------------------|\n| `x`       | positive integer or a vector                                                            |\n| `size`    | non-negative integer, number of items to choose                                         |\n| `replace` | logical - should sampling be with replacement                                           |\n| `prob`    | a vector of probability weights for obtaining the elements of the vector being sampled. |\n\n: {tbl-colwidths=\"\\[20, 80\\]\"}\n\n## `sample()` examples\n\n::: panel-tabset\n## `sample(n)`\n\nA random permutation of 1, ..., n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3 10  2  8  6  9  1  7  5  4\n```\n\n\n:::\n:::\n\n\nSee also `sample.int()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nsample.int(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  3 10  2  8  6  9  1  7  5  4\n```\n\n\n:::\n:::\n\n\n\n## `sample(x)`\n\nA random permutation of `x` for `length(x) > 1`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(10)\nalph5 <- LETTERS[1:5]\nsample(alph5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C\" \"A\" \"B\" \"E\" \"D\"\n```\n\n\n:::\n:::\n\n\n## `sample(x, replace = TRUE)`\n\nA bootstrap sample\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(10)\nsample(alph5, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C\" \"A\" \"B\" \"D\" \"C\"\n```\n\n\n:::\n:::\n\n\n## `sample(x, size = n)`\n\nSample `n` items from `x` without replacement\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(10)\nsample(alph5, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C\" \"A\" \"B\"\n```\n\n\n:::\n:::\n\n\n## `sample(x, n, replace = TRUE)`\n\nSample `n` items from `x` with replacement\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nsample(alph5, 3, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"A\" \"D\" \"A\"\n```\n\n\n:::\n:::\n\n\n## `sample(x, n, replace = TRUE, prob = probs)`\n\nProbability sample of `n` items from `x`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- 1:4\nprobs <- c(1/2, 1/3, 1/12, 1/12)\n\nset.seed(1)\nsamp <- sample(x, 100, replace = TRUE, prob = probs)\ntable(samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsamp\n 1  2  3  4 \n52 34 10  4 \n```\n\n\n:::\n:::\n\n:::\n\n## Distributions in R\n\nMany statistical distributions are built into R. Each has a root name, e.g. `norm` for the normal distribution.\n\nThere are four functions for each distribution, with different letters prefixing the root name:\n\n-   `p` for *probability*, the cumulative distribution function\n-   `q` for *quantile*, the inverse c.d.f.\n-   `d` for *density*, the density function, p.d.f.\n-   `r` for *random*, a random sample from the distribution.\n\nSo, for the normal distribution, we have the functions `pnorm`, `qnorm`, `dnorm`, `rnorm`.\n\n## Distributions and parameterisations {.smaller60}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Distribution      |Base name |Parameters          |\n|:-----------------|:---------|:-------------------|\n|beta              |`beta`    |`shape1`, `shape2`  |\n|binomial          |`binom`   |`size`, `prob`      |\n|Cauchy            |`cauchy`  |`location`, `scale` |\n|chi-squared       |`chisq`   |`df`                |\n|exponential       |`exp`     |`rate`              |\n|F                 |`f`       |`df1`, `df2`        |\n|gamma             |`gamma`   |`shape`, `rate`     |\n|geometric         |`geom`    |`p`                 |\n|hypergeometric    |`hyper`   |`m`, `n`, `k`       |\n|log-normal        |`lnorm`   |`meanlog`, `sdlog`  |\n|logistic          |`logis`   |`location`, `scale` |\n|negative binomial |`nbinom`  |`size`, `prob`      |\n|normal            |`norm`    |`mean`, `sd`        |\n|Poisson           |`pois`    |`lambda`            |\n|Student t         |`t`       |`df`                |\n|uniform           |`unif`    |`min`, `max`        |\n|Weibull           |`weibull` |`shape`, `scale`    |\n\n\n:::\n:::\n\n\n## Generating random samples from a distibution\n\nTo obtain a sample of size `n`, use the `r` function for the distribution with the first argument `n`, and subsequent arguments the parameters for that distribution. The parameters often have default values. E.g.\n\n|                          |                                                                       |\n|--------------------------|-----------------------------------------------------------------------|\n| `runif(n, min=0, max=1)` | Uniform                                                               |\n| `rnorm(n, mean=0, sd=1)` | Normal                                                                |\n| `rexp(n, rate=1)`        | Exponential with mean 1/rate                                          |\n| `rt(n, df)`              | t with df degrees of freedom                                          |\n| `rbinom(n, size, prob)`  | Binomial, success in `size` trials with probability of success `prob` |\n\n: {tbl-colwidths=\"\\[50, 50\\]\"}\n\n[Table from <https://www.johndcook.com/blog/distributions_r_splus/>]{.smaller60}\n\n## Your turn!\n\nSometimes `sample()` can lead to nasty surprises.\n\nConsider the following code. Which line(s) might cause a problem?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- 1:10\nsample(x[x > 7])\nsample(x[x > 8])\nsample(x[x > 9])\nsample(x[x > 10])\n```\n:::\n\n\nRun the code to see what happens.\n\n. . .\n\n:::{.callout-warning}\nYou need to be careful when using `sample()` programmatically (i.e. in your function or simulation).\n\nSee `?sample` for a safer version and check you understand how the proposed `resample()` function in the examples works.\n:::\n\n# Optimisation {.inverse}\n\n## The optimisation problem\n\n-   Given a function $f(x)$, what value of $x$ makes $f(x)$ as small or large as possible?\n-   In a statistical context, $x$ will usually be the parameters of a model, and $f(x)$ either the model likelihood to be maximised or some measure of discrepancy between data and predictions to be minimised\n-   The optimal set of parameters will give the *best fit*\n-   Only need to consider *small* as $-f(x)$ is *large* when $f(x)$ is *small*.\n-   We consider here *general-purpose optimisers*\n\n## Local and global minima\n\n-   The (negative of the) likelihood for the General Linear Model (and that for many other linear models) is well-behaved: it has a single, global minimum.\n-   For more complicated models there may be many local minima.\n-   Finding a global minimum is difficult, and not always important. Only if local minima are widely separated in parameter space are they likely to invalidate our conclusions.\n-   We will concentrate on methods of finding local minima. Check for different local minima by altering the initial values, algorithm used, or other parameters of the fitting process.\n\n## Univariate optimisation\n\n`optimize` (or `optimise`) finds a (possibly local) mimimum of a function in a specified interval with respect to its first argument.\n\n-   Function to be minimised is the first argument to `optimize`\n-   Can pre-specify the function or include it in the command.\n\n## Univariate optimisation: example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf <- function(x, a) {\n  (x - a)^2\n}\n\nxmin <- optimize(f, interval = c(0, 1), a = 1/3)\n\n# or\n\nxmin <- optimize(function(x, a) {(x - a)^2}, \n                 interval = c(0, 1), a = 1/3)\n\nxmin\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$minimum\n[1] 0.3333333\n\n$objective\n[1] 0\n```\n\n\n:::\n:::\n\n\nNote how the (fixed) parameter `a` is passed into `f`.\n\n## Other `optimize()` arguments\n\n:::{.smaller90}\n-   An interval within which to search must be specified, either by `interval` or by `upper` and `lower`\n\n-   To *maximise* set `maximum = TRUE`\n\n-   Accuracy can be set using the `tol` argument\n\n-   Note the order of arguments: `optimize(f, interval, ..., lower, upper, maximum, tol)`\n\n-   The `...` can be named or unnamed and will be passed to `f`\n\n-   Arguments after the `...` must be specified by names.\n\n-   `optimize` returns a list with two items:\n\n    -   `minimum`: the value of `x` at which `f(x)` is minimised\n    -   `objective`: the value of `f(x)` at `x = minimum`\n:::\n\n## General optimisation\n\n-   In more than one dimension the problem is harder.\n-   R has several different functions: most flexible is `optim()` which includes several different algorithms.\n-   Algorithm of choice depends on how easy it is to calculate derivatives for the function. Usually better to supply a function to calculate derivatives, but may be unnecessary extra work.\n-   Ensure the problem is scaled so that unit change in any parameter gives approximately unit change in objective.\n\n## `optim()` methods\n\n::: panel-tabset\n## Nelder-Mead\n\n-   The default method\n-   Basic idea: for a function with `n` parameters, choose a polygon with `n+1` vertices. At each step, alter vertex with minimum `f(x)` to improve the objective function, by *reflection*, *expansion* or *contraction*\n-   Does not use derivative information\n-   Useful for non-differentiable functions\n-   May be rather slow\n\n## BFGS\n\n-   A quasi-Newton method: builds up approximation to Hessian matrix from gradients at start and finish of steps\n-   Uses the approximation to choose new search direction\n-   Performs line search in this direction\n-   Update term for the Hessian approximation is due to Broyden, Fletcher, Goldfarb and Shanno (proposed separately by all four in 1970)\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n-   If dimension is large, the matrix stored may be very large\n\n## CG\n\n-   A conjugate gradient method: chooses successive search directions that are analogous to axes of an ellipse\n-   Does not store a Hessian matrix\n-   Three different formulae for the search directions are implemented: Fletcher-Reeves, Polak-Ribiere or Beale-Sorenson\n-   Less robust than BFGS method\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n\n## L-BFGS-B\n\n-   A limited memory version of BFGS\n-   Does not store a Hessian matrix, only a limited number of update steps for it\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n-   Can restrict the solution to lie within a \"box\", the only method of `optim()` that can do this\n\n## SANN\n\n-   A variant of simulated annealing A stochastic algorithm\n-   Accepts changes which increase the objective with positive probability (when minimising!)\n-   Does not use derivative information\n-   Can be very slow to converge but may find a 'good' solution quickly\n\n## Brent\n\n-   An interface to `optimize()`\n-   Only for use with one dimensional problems\n-   Included for use inside other functions where only method can be specified, not the function to be used.\n:::\n\n## How to use `optim()` {.smaller75}\n\n`optim(par, fn, gr=NULL, ..., method=c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower=-Inf, upper=Inf, control=list(), hessian=FALSE)`\n\n|                  |                                                                                            |\n|-------------------------------------------------------|-----------------|\n| `par`            | starting values of the parameters                                                          |\n| `fn`             | function to be optimised (supply as for `optimize`)                                        |\n| `gr`             | function to calculate the derivative, only relevant for methods \"BFGS\", \"CG\" or \"L-BFGS-B\" |\n| `…`              | other parameters for (both) `fn` and `gr`                                                  |\n| `method`         | algorithm to use                                                                           |\n| `lower`, `upper` | vector of limits for parameters (only allowed it `method=\"L-BFGS-B\"`                       |\n| `control`        | control options (see next slide)                                                           |\n| `hessian`        | logical: whether to return a hessian estimate calculated by finite differences             |\n\n: {tbl-colwidths=\"\\[20, 80\\]\"}\n\n## `optim()`: control options\n\nThere are very many. The most important are:\n\n|            |                                                                                                                                                                   |\n|------------|-----------------------------------------------------------|\n| `trace`    | A positive integer: higher values give more information                                                                                                           |\n| `fnscale`  | An overall scaling: if negative, maximisation will be performed                                                                                                   |\n| `parscale` | A vector of scalings for the parameters                                                                                                                           |\n| `maxit`    | Maximum number of iterations to be performed. May be used to terminate unsuccessful attempts. Also used to perform one or two steps if convergence is unimportant |\n| `type`     | Used to select formula for \"CG\" method                                                                                                                            |\n\n## `optim()`: components of return value\n\n|               |                                                                                                                                      |\n|--------------|----------------------------------------------------------|\n| `par`         | best set of parameters                                                                                                               |\n| `value`       | value of `fn` corresponding to `par`                                                                                                 |\n| `counts`      | number of calls to `fn` and `gr`: excludes calls for purposes of approximating derivatives or Hessian                                |\n| `convergence` | error code: 0=success, 1=`maxit` reached, 10=degeneracy of Nelder-Mead simplex, 51=warning from \"L-BFGS-B\", 52=error from \"L-BFGS-B\" |\n| `message`     | further information, if any                                                                                                          |\n| `hessian`     | if requested, a symmetric matrix estimate of the Hessian at the solution    \n\n|\n\n## Your turn! Q1\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfw <- function (x) {\n    10*sin(0.3*x)*sin(1.3*x^2) + 0.00001*x^4 + 0.2*x+80\n}\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n- Use `optim()` to find an approximate global minimum\n\n- Use `optim()` again to improve locally (aiming for an objective of 67.46773).\n\n::: {.notes}\nthis occurs when `$minimum` x = -15.81515   \n:::\n\n\n## Your turn! Q2 {.smaller90}\n\n- Take a look at the introduction to the Wikipedia page for [Rosenbrock's banana function](https://en.wikipedia.org/wiki/Rosenbrock_function){target=\"_blank\"}\n\n- Copy this code for the `fn` and `gr`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfn <- function(x) {   \n    x1 <- x[1]\n    x2 <- x[2]\n    100 * (x2 - x1 * x1)^2 + (1 - x1)^2\n}\n\ngr <- function(x) { \n    x1 <- x[1]\n    x2 <- x[2]\n    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),\n       200 *      (x2 - x1 * x1))\n}\n```\n:::\n\n\n- Using a start value of `c(0, 0)`, experiment with the `optim()` function to see which methods converge\n\n# End matter {.inverse}\n\n## Resources\n\nThis material is reproduced in large part directly from the APTS 2013/14 resources by Ruth Ripley:\n\n-   <https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf>\n\n## License\n\nLicensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/){target=\"_blank\"}).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}